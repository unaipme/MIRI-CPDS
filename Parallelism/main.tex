\documentclass[10pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{pifont}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{minted}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{graphicx}


\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\title{CPDS: OpenMP cheatseet}

\begin{document}

\footnotesize

\begin{center}
     \Large{\textbf{CPDS: OpenMP cheatsheet}} \\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\section{Directives syntax}
\par
In C/C++, OpenMP is used through compiler directives. The syntax is ignored if the compiler does not know OpenMP.
\begin{minted}{c}
#pragma omp construct [clauses]
\end{minted}
\section{Memory model}
\par
OpenMP defines a relaxed memory model. Threads can see different values for the same variable (Variables can be shared or private to each thread). Memory consistency is only guaranteed at specific points.
\section{Constructs}
\subsection{The \texttt{parallel} construct}
\begin{minted}{c}
#pragma omp parallel [clauses]
    // structured block...
\end{minted}
\par
In that sense, directives work just like \texttt{if} statements. If the conditional block of the \texttt{if} is just one block, it can be written just like this:
\begin{minted}{c}
if (something) doIt();
\end{minted}
\par
This syntax can be used for \textit{joining} different blocks even though they are not in the same line, as shown below. The \texttt{for} loop will only run if the condition \texttt{something} is true, even though we didn't enclose the conditional block between curly braces (\texttt{\{ \}}).
\begin{minted}{c}
if (something)
for (;;) {
    // do something...
}
\end{minted}
\par
The same goes for OpenMP directives. The following two pieces of code are equivalent.
\begin{multicols}{2}
\begin{minted}{c}
#pragma omp parallel
{
    for (;;) {
        // do something...
    }
}
\end{minted}
\begin{minted}{c}
#pragma omp parallel
for (;;) {
    // do something...
}
//
\end{minted}
\end{multicols}
\subsubsection{Number of threads}
\par
The \texttt{nthreads-var} ICV (internal variable) is used to determine the number of threads to be used for parallel regions. It's a list of positive integer values. For each occurrence of \texttt{parallel}, the first element is popped from the list.
\par
The value of the variable can be set with the environment variable \texttt{OMP\_NUM\_THREADS}, through the function \texttt{omp\_set\_num\_threads}, or by defining the directive \texttt{num\_threads}.
\begin{minted}{c}
unsigned int N = ...;
omp_set_num_threads(N);
#pragma omp parallel num_threads(N)
\end{minted}
\subsection{The \texttt{if} clause}
\par
It can be used to conditionally run regions parallelly. When the condition evaluates to false, the region is run on one only thread.
\begin{minted}{c}
int iterations = ...;
#pragma omp parallel if (iterations > 10)
\end{minted}
\subsection{Example: amount of threads}
\begin{minted}[breaklines]{c}
void main() {
    #pragma omp parallel
    // ...
    omp_set_num_threads(2);
    #pragma omp parallel
    //...
    #pragma omp parallel num_threads(random() % 4 + 1) if(0)
    //...
}
\end{minted}
\par
\textbf{How many threads are used in each parallel region?} The first region uses the default amount of threads. The second region will use 2 threads. The third region will not run parallely, as the \texttt{if} will always evaluate to false.
\section{Data-sharing attributes}
\subsection{Shared}
\par
When a variable is marked as \texttt{shared}, all threads use the same variable, this is, access the same location in memory. By default, variables are implicitly shared.
\subsection{Private}
\par
When a variable is marked as \texttt{private}, it means that each thread has a different variable with an originally undefined value that can be accessed without any kind of synchronization.
\subsection{Firstprivate}
\par
Whan a variable is marked as \texttt{firstprivate}, it means that each thread has a different variable initialized to the original value that can be accessed without any kind of synchronization.
\subsection{Example: Data-sharing}
\begin{minted}{c}
int x = 1;
#pragma omp parallel XXXXXX num_threads(2)
{
    x++;
    printf("%d\n", x);
}
printf("%d\n", x);
\end{minted}
\textbf{What gets printed on screen if \texttt{XXXXXX} is \texttt{shared(x)}, \texttt{private(x)} or \texttt{firstprivate(x)}?} Note that the final \texttt{printf} falls out of the \texttt{parallel} construct.
\par
When \texttt{x} is shared, it is difficult to know. A race condition will occur. One of the threads will run \texttt{x++} before the other. So, the two first lines will be a 2 and a 3 (with undetermined order), followed by a 3.
\par
When \texttt{x} is private, the result is undefined, as the variables that the threads will be able to access are undefined.
\par
When \texttt{x} is firstprivate, it will print 2 twice, because each thread modifies a private variable initialized to 1. The original variable remains untouched, so it's just where it was initialized.
\subsubsection{Try it yourself}
\par
Compile the source \texttt{data-sharing.c} with \texttt{make data-sharing} and run it with \texttt{./data-sharing}. Because of the compilation options, the uninitialized variables will be set to 0 by default. Thus, the example with private variables will print 1 three times.
\subsection{Example: Computation of $\pi$}
\par An approximation of $\pi$ can be calculated with the following sequential code.
\begin{minted}[xleftmargin=20pt, linenos]{c}
static long num_steps = 100000;
double step;

void main() {
    int i;
    double x, pi, sum = 0.0;
    
    step = 1.0 / (double) num_steps;
    
    for (i = 1; i <= num_steps; i++) {
        x = (i - 0.5) * step;
        sum = sum + 4.0 / (1.0 + x * x);
    }
    pi = step * sum;
}
\end{minted}
\par
Say the goal is to parallelize the code. The \texttt{for} loop can be parallelized. How would the \texttt{\#pragma} construct affect the data sharing? Variables \texttt{i}, \texttt{x}, and \texttt{sum}, are accessed and written in the loop, and, by default, these variables are shared. Not having a proper data sharing design would alter the result.
\par
As opposed to the other variables, \texttt{sum} is only read just before writing it by means of an addition. Additions are commutative ($3+2=2+3$) and associative ($(2+3)+4=2+(3+4)$), so \texttt{sum} can be shared. If \texttt{sum} was to be private, the initial value would be undefined, this is, the sum would not start from 0. If it was to be firstprivate, the results of each thread would need to be collected somehow after the fact.
\par
Meanwhile, different values of \texttt{i} and \texttt{x} can be read in crucial moments of the calculation. Specifically, the addition in line 12 may access a different value of \texttt{x} than the one calculated by the same thread a line before.
\par
The solution would be to make \texttt{i} and \texttt{x} private, as shown in the following code snippet:
\begin{minted}{c}
#pragma omp parallel private(i, x)
for (i = 1; i <= num_steps; i++) {
    x = (i - 0.5) * step;
    sum = sum + 4.0 / (1.0 * x * x);
}
pi = step * sum;
\end{minted}
\par
Recall that only the \texttt{for} loop will be parallelized. The last assignment (\texttt{pi = step * sum}) is run sequentially.
\section{Some API calls}
\begin{itemize}
    \item \texttt{int omp\_get\_num\_threads()} returns the number of threads in the current team.
    \item \texttt{int omp\_get\_thread\_num()} Returns the id of the thread in the current team. Goes from 0 to \texttt{omp\_get\_thread\_num() - 1}
    \item 
\end{itemize}
\section{Thread synchronization}
\par
OpenMP follows a shared memory model. Threads communicate by sharing variables. Unintended sharing of data may cause race conditions. Threads need to synchronize to impose some ordering in their sequence of actions.
\subsection{Thread barrier}
\begin{minted}{c}
#pragma omp barrier
\end{minted}
\par
Threads cannot proceed past a barrier point until all the parallel threads reach the barrier. Some constructs, such as \texttt{parallel}, have an implicit barrier at the end.
\begin{minted}[xleftmargin=20pt, linenos]{c}
#pragma omp parallel
{
    foo();
    #pragma omp barrier
    bar();
}
\end{minted}
\par
The explicitly defined barrier in line 4 forces all threads to finish running \texttt{foo()} before running \texttt{bar()}. At the same time, the end of the parallel region at line 6 implicitly means that all threads must finish running \texttt{bar()} before the code keeps running sequentially.
\subsection{Exclusive access: critical construct}
\begin{minted}{c}
#pragma omp critical [(name)]
    // structured block
\end{minted}
\par
Makes a parallel region accessible to only one thread at any given time, this is, mutual exclusion. Unless explicitly named, all critical regions are the same.
\begin{minted}{c}
#pragma omp parallel
{
    foo();
    #pragma omp critical
    bar();
    #pragma omp critical
    baz();
}
\end{minted}
\par
In the example above, there are two different critical regions. Nevertheless, as none of them are named, only one thread can run either one of the regions. The two of them will never be run at the same time. This can be fixed by naming either of the regions, as shown below.
\begin{minted}{c}
#pragma omp parallel
{
    foo();
    #pragma omp critical part1
    bar();
    #pragma omp critical part2
    baz();
}
\end{minted}
\subsection{Exclusive access: atomic construct}
\begin{minted}{c}
#pragma omp atomic [ update | read | write ]
    // expression
\end{minted}
The construct ensures that a specific storage location is accessed in a mutually exclusive way, avoiding the possibility of multiple, simultaneous reading and writing threads. It is usually more efficient than a \texttt{critical} construct. There are three types of atomic accesses:
\begin{itemize}
    \item Updates: \texttt{x++, x -= foo()}. An operation that reads from and writes in the same memory space. (These are those operations that can be represented as \texttt{+=}, \texttt{--}, \texttt{<<=} or \texttt{/=}, among others)
    \item Reads: \texttt{value = *p}. In this case, the value of \texttt{p} is being directly read.
    \item Writes: \texttt{*p = value}. In this case, the value of \texttt{p} is being directly written on.
\end{itemize}
\subsection{Reduction clause}
\begin{minted}{c}
#pragma omp parallel [...] reduction(operator:variable)
    // block...
\end{minted}
\par
Reduction is a pattern where all threads accumulate values into a single variable. Valid operators are \texttt{+}, \texttt{-}, \texttt{*}, \texttt{|} (bitwise OR), \texttt{||} (logical OR), \texttt{\&} (bitwise AND), \texttt{\&\&} (logical AND) and \texttt{\^} (bitwise XOR). The compiler implicitly creates a properly initialized private copy of the variable for each thread and, at the end of the region, it takes care of safely updating it with the partial solutions.
\begin{minted}{c}
#pragma omp parallel private(x, i, id) reduction(+:sum)
{
    id = omp_get_thread_num();
    for (i = id + 1; i <= num_steps; i += NUM_THREADS) {
        x = (i - 0.5) * step;
        sum = sum + 4.0 / (1.0 + x * x);
    }
}
pi = sum * step;
\end{minted}
\par
In the example above, each thread will have a private copy of \texttt{sum}, probably initialized to 0. Threads will calculate the sum of the respective iterations they run, depending on their ID. As additions are commutative and associative, the results can be aggregated after the barrier without any loss of information.
\subsection{Locks}
\par
OpenMP provides lock primitives for low-level synchronization. Locks work much like critical regions. They are acquired before entering a mutual exclusion region and must be released afterwards.
\begin{minted}[xleftmargin=20pt, linenos]{c}
#include <omp.h>
void foo() {
    omp_lock_t lock;
    omp_init_lock(&lock);
    #pragma omp parallel
    {
        omp_set_lock(&lock);
        // mutual exclusion region
        omp_unset_lock(&lock);
    }
    omp_destroy_lock(&lock);
}
\end{minted}
\par
Note, in the example above, that locks need initialization and destruction, which is taken care of with the functions on lines 4 and 11, respectively.
\section{Memory consistency}
\begin{minted}{c}
#pragma omp flush (list)
\end{minted}
\par
It enforces consistency between the temporary view and memory for those variables in the list. Synchronization constructs (implicit or explicit) have an associated flush operation.
\section{Loop parallelism}
\subsection{The worksharing concept}
\par
Worksharing constructs divide the execution of a code region among the members of a team. Threads cooperate to do some work. It is a better way to split work than thread IDs, and has a lower overhead than tasks, even though it's less flexible.
\subsection{The \texttt{for} construct}
\begin{minted}{c}
#pragma omp for [clauses]
    for (init-expr; test-expr; inc-expr)
\end{minted}
\par
The iterations of the loops will be divided among the threads. Loop iterations must be independent and it must follow a shape that allows induction of amount of iterations. Valid types for inductions are integers, pointers and random access iterators (C++). This inducted variable is private.
\begin{minted}{c}
void main() {
    int i, id;
    double x, pi, sum;
    
    step = 1.0 / (double) num_steps;
    #pragma omp parallel for private(x) reduction(+:sum)
    for (i = 1; i <= num_steps; i++) {
        x = (i - 0.5) * step;
        sum  = sum + 4.0 / (1.0 + x * x);
    }
    pi = sum * step;
}
\end{minted}
\par
The \texttt{for} construct automatically detects \texttt{i} as the variable for iteration control. OpenMP makes it private and then distributes iterations among the threads. \texttt{x} still must be marked as private, race conditions can still occur. \texttt{sum} will have a different copy for each of the threads, initialized to 0. Thanks to the \texttt{reduct} clause, all the local results of \texttt{sum} per thread will be added after the synchronization.
\subsection{The \texttt{schedule} clause}
\par
This clause determines which iterations are executed by each thread. If no clause is present, the implementation should take care of this. There are several options:
\begin{itemize}
    \item \texttt{static}. The iteration space is broken in chunks of size $\frac{N}{\text{num\_threads}}$. This is, the iterations are evenly divided across the threads. Chunks are assigned to threads in Round-Robin fashion.
    \item \texttt{static,N} (interleaved). The iteration space is broken in chunks of size $N$. Then this chunks are scheduled to threads in Round-Robin fashion.
\end{itemize}
\par
The overall characteristics of static scheduling are low overhead, (usually) good locality and the possibility of load imbalance problems.
\begin{itemize}
    \item \texttt{dynamic,N}. Threads dynamically grab chunks of $N$ iterations until all iterations have been executed. $N=1$ by default.
    \item \texttt{guided,N}. The size of the chunks decreases as the threads grab iterations, but it is at least of size $N$, $N=1$ by default.
\end{itemize}
\par
These dynamic schedules result in higher overhead, not very good locality (usually) but they can solve imbalance problems.
\subsection{The \texttt{nowait} clause}
\par
The \texttt{nowait} clause removes the implicit \texttt{barrier} from the end of a parallel region. This allows to overlap the execution of non-dependent loops/tasks/worksharings.
\begin{minted}{c}
#pragma omp for nowait
for (i = 0; i < n; i++)
    v[i] = 0;
#pragma omp for
for (i = 0; i < n; i++)
    a[i] = 0;
\end{minted}
\par
In the example above, the work of the second loop is independent from the work of the first loop, hence there is no need for the threads to wait for synchronization after running the first loop. The \texttt{nowait} construct allows this.
\begin{minted}{c}
#pragma omp for schedule(static, 2) nowait
for (i = 0; i < n; i++)
    v[i] = 0;
#pragma omp for schedule(static, 2)
for (i = 0; i < n; i++)
    a[i] = v[i] * v[i];
\end{minted}
\par
In the example above, a static scheduling policy is defined. 2 different chunks will be created with the halves of the iterations of each \texttt{for} loop. So, even though the second loop is directly dependent on the results of the first loop, each of the threads will be dependent only of the elements calculated in the iterations run by themselves. The \texttt{nowait} construct allows the threads to continue running when they finish with the first loop.
\subsection{The \texttt{collapse} clause}
\par
The \texttt{collapse} clause allows to distribute work from a set of $n$ nested loops. The loops must be perfectly nested. The nest must traverse a rectangular iteration space.
\begin{minted}{c}
#pragma omp for collapse(2)
for (i = 0; i < N; i++) 
    for (j = 0; j < M; j++)
        foo(i, j);
\end{minted}
\par
In the example above, the loops of $i$ and $j$ are folded and iterations distributed among all threads, and both variables are privatized.
\subsection{The \texttt{single} construct}
\begin{minted}{c}
#pragma omp single [clauses]
    // structured block
\end{minted}
\par
The construct makes the structured block to be run in only one thread. The clauses can be \texttt{private}, \texttt{firstprivate} and \texttt{nowait}. There is an implicit \texttt{barrier} at the end.
\section{Task parallelism}
\subsection{Task parallelism model}
\par
Tasks are work units whose execution may be deferred, but they can also be executed immediately. Threads, separated in teams, cooperate to execute them.
\subsection{Task creation}
\par
Parallel regions create tasks. One implicit task is created and assigned to each thread. Each thread that encounters a \texttt{task} construct packages the code and data and creates a new explicit task.
\subsubsection{Explicit task creation}
\begin{minted}{c}
#pragma omp task [clauses]
    // structured block
\end{minted}
\par
Where some possible clauses are \texttt{shared}, \texttt{private}, \texttt{firstprivate}, \texttt{if(expr)}, \texttt{final(expr)} and \texttt{mergeable}.
\begin{minted}{c}
void traverse_list(List l) {
    Element e;
    for (e = l->first; e; e = e -> next) {
        #pragma omp task
        process(e); // e is firstprivate by default
    }
}
\end{minted}
\par
The code above defines a task block, the \texttt{process} function. But tasks are useless if they are not defined within a parallel region. Let's complete the code.
\begin{minted}{c}
List l;

#pragma omp parallel
traverse_list(l);

void traverse_list(List l) {
    // ...
}
\end{minted}
\par
In the code above the \texttt{traverse\_list} function is going to be run by as many threads as defined. All of the threads will run all of the tasks, so the traversal of the list will be calculated by all threads. Execution is not actually parallelized.
\begin{minted}{c}
List l;

#pragma omp parallel
#pragma single
traverse_list(l);

void traverse_list(List l) {
    // ...
}
\end{minted}
\par
With the addition of the construct \texttt{single}, only one of the threads will proper run the \texttt{traverse\_list} function. This thread will create the tasks. The rest of threads (and the first thread, once the task generation is complete) will run the tasks in parallel.
\subsection{Default task data-sharing attributes}
\par
When no data clauses are specified, global variables are shared, variables declared within the scope of a task are private, and the rest are firstprivate, except when a \texttt{shared} attribute is inherited.
\begin{minted}{c}
int a;
void foo() {
    int b, c;
    #pragma omp parallel shared(b)
    #pragma omp parallel private(b)
    {
        int d;
        #pragma omp task
        {
            int e;
        }
    }
}
\end{minted}
\par
In the code above, the variables are:
\begin{itemize}
    \item \texttt{a} is global, and so shared by default.
    \item \texttt{b} is \textcolor{red}{firstprivate}.
    \item \texttt{c} is shared because, in the context of the task, it is effectively global.
    \item \texttt{d} is firstprivate, because it is declared within the parallel region and it its attribute is not explicitly defined.
    \item \texttt{e} is private, because it is declared withing the scope of the task.
\end{itemize}
\subsection{Immediate task execution}
\subsubsection{The \texttt{if} clause}
\par
When an \texttt{if} clause is present on a task construct, and its expression evaluates to \texttt{false}, an undeferred task is generated. The encountering thread must suspend the current task region, the execution of which cannot be resumed until the generated task is completed. This allows implementations to optimize task creation.
\subsubsection{The \texttt{final} clause}
\par
If the expression of a \texttt{final} clause evaluates to \texttt{true}, then the generated task and its children will be final and included. Execution of included tasks is done immediately after the generating task. So, all tasks created within a \texttt{final} region will be run sequentially and immediately, and if more tasks are created, they will also be final and have the same final task generating capabilities.
\par
When a \texttt{mergeable} clause is present on a task construct, and the generated task is an included task, the implementation may generate a merged task instead (i.e. no task and context creation for it).
\section{Task synchronization}
\par
There are two types of task barriers:
\begin{itemize}
    \item \texttt{taskwait}: Suspends the current task waiting on the completion of child tasks of the current task. This construct is stand-alone.
    \item \texttt{taskgroup}: Suspends the current task at the end of a structured block waiting on completion of child tasks of the current task and their descendent tasks.
\end{itemize}
\subsection{\texttt{taskwait}}
\begin{minted}{c}
#pragma omp task {}     // T1
#pragma omp task        // T2
{
    #pragma omp task {} // T3
}
#pragma omp task {}     // T4

#pragma omp taskwait
\end{minted}
\par
With the code above, only tasks 1, 2 and 4 are guaranteed to have finished after \texttt{taskwait}, as it only guarantees the completion of child tasks of the current task.
\begin{minted}{c}
int fib(int n) {
    int i, j;
    
    if (n < 2) return n;
    
    #pragma omp task shared(i) final(n <= THOLD) mergeable
    i = fib(n - 1);
    #pragma omp task shared(j) final(n <= THOLD) mergeable
    j = fib(n - 2);
    
    #pragma omp taskwait
    return i + j;
}
\end{minted}
\subsection{\texttt{taskgroup}}
\begin{minted}{c}
#pragma omp task {}         // T1
#pragma omp taskgroup {
    #pragma omp task        // T2
    {
        #pragma omp task {} // T3
    }
    #pragma omp task {}     // T4
}
\end{minted}
\par
With the code above, only tasks 2 to 4 are guaranteed to have finished after the \texttt{taskgroup} clause, as it guarantees the completion of all child tasks, and their child tasks recursively.
\section{Data sharing inside tasks}
\par
In addition one can use \texttt{critical} and \texttt{atomic} to synchronize the access to shared data inside tasks.
\begin{minted}{c}
void process(Element e) {
    // ...
    #pragma omp atomic
    solutions_found++;
    // ...
}
\end{minted}
\section{Task dependencies}
\par
Dependence between sibling tasks can be expressed as follows:
\begin{minted}{c}
#pragma omp task    [depend (in : var_list)]
                    [depend (out : var_list)]
                    [depend (inout : var_list)]
\end{minted}
\par
The exact dependence between tasks is inferred from the dependence type and the items in the variable list. This list may include array sections.
\begin{itemize}
\item Tasks with the \texttt{in} dependence-type will be dependent of all previously generated sibling tasks that reference, at least, one of the items in the variable list in an \texttt{out} or \texttt{inout} dependence-type list.
\item Tasks with the \texttt{out} or \texttt{inout} dependence-types will be dependent on all the previously generated tasks mentioning, at least, one of the items in the variable list.
\end{itemize}
\begin{minted}[breaklines]{c}
#pragma omp parallel private(i, j)
#pragma omp single
{
    for (i = 1; i < n; i++) {
        for (j = 1; j < n; j++) {
            #pragma omp task
                    depend(in : block[i - 1], block[i][j - 1])
                    depend(out : block[i][j]
        foo(i, j);
        }
    }
}
\end{minted}
\vfill
\hrule
~\\
Unai Perez Mendizabal \textcopyright \href{https://github.com/unaipme}{https://github.com/unaipme}
\end{multicols}
\newpage\begin{center}
 \Large{\textbf{CPDS: MPI cheatsheet}} \\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}
\section{Basic concepts}
\begin{itemize}
    \item \textbf{Communicator}: It is the context for a communication operator. Messages are received within the context from where they were sent. Messages sent to different contexts do not interfere. The global context is \texttt{MPI\_COMM\_WORLD}
    \item \textbf{Process group}: Set of processes that share a communication context.
\end{itemize}
\section{Main environmental functions}
\subsection{\texttt{MPI\_INIT}}
\begin{minted}{c}
int MPI_Init(int * argc_ptr,     // in
            char** argv_ptr[] ); // in
\end{minted}
\par
It initializes the MPI environment. All MPI programs must call this routine once and only once before any other MPI routines.
\subsection{\texttt{MPI\_COMM\_SIZE}}
\par
\begin{minted}{c}
int MPI_Comm_size(MPI_Comm comm, // in
                int* size );     // out
\end{minted}
\par
It returns the number of processors (\texttt{size}) in the group associated to communicator \texttt{comm}.
\subsection{\texttt{MPI\_COMM\_RANK}}
\begin{minted}{c}
int MPI_Comm_rank(MPI_Comm comm, // in
                int* rank );     // out
\end{minted}
\par
It returns the identifier of the local process in the group associated with communicator \texttt{comm}. The identifier (\texttt{rank}) of the process is within range [0, size-1].
\subsection{\texttt{MPI\_FINALIZE}}
\begin{minted}{c}
int MPI_Finalize(void);
\end{minted}
\par
It terminates all MPI processing. This routine MUST be the last MPI call, all pending communications involving a process must have completed before the process calls \texttt{MPI\_FINALIZE}.
\subsection{\texttt{MPI\_ABORT}}
\begin{minted}{c}
int MPI_Abort(MPI_Comm comm, // in
            int errorcode ); // in
\end{minted}
\par
Forces all processes of an MPI job to terminate.
\section{Barrier synchronization}
\begin{minted}{c}
int MPI_Barrier(MPI_Comm comm); // in
\end{minted}
\par
Blocks all processes in communicator \texttt{comm}'s context until all processes have reached the point and called the barrier.
\begin{minted}{c}
#include "mpi.h"

int rank;
int nproc;

int main(int argc, char* argv[] ) {
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nproc);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    // Ensure all processes arive here before timing
    MPI_Barrier(MPI_COMM_WORLD);
    // Something to do in local
    
    MPI_Finalize();
}
\end{minted}
\section{Point-to-point communication}
\subsection{Blocking operations}
\par
Return form the procedure indicates the user is allowed to reuse resources specified in the call. In other words, the execution will wait for the communication to complete.
\subsubsection{\texttt{MPI\_SEND}}
\begin{minted}{c}
int MPI_Send(void* buf,             // in
            int count,              // in
            MPI_Datatype datatype,  // in
            int destination,        // in
            int tag,                // in
            MPI_Comm comm );        // in
\end{minted}
\par
Performs a blocking send operation. The message can be received by either \texttt{MPI\_RECV} (blocking call) or \texttt{MPI\_IRECV} (non-blocking call). The three last parameters (\texttt{destination}, \texttt{tag} and \texttt{comm}) are called the message envelope; it is the information used to distinguish messages and selectively receive them.
\subsubsection{\texttt{MPI\_RECV}}
\begin{minted}{c}
int MPI_Recv(void* buf,             // out
            int count,              // in
            MPI_Datatype datatype,  // in
            int source,             // in
            int tag,                // in
            MPI_Comm comm,          // in
            MPI_Status* status );   // out
\end{minted}
\par
Performs a blocking receive operation. The message received must be less than or equal to the length of the receive buffer \texttt{buf}. It can receive a message sent by either \texttt{MPI\_SEND} or \texttt{MPI\_ISEND}. The message envelope this time is \texttt{source}, \texttt{tag} and \texttt{comm}.
\subsubsection{Example}
\begin{minted}[breaklines]{c}
#include "mpi.h"

int rank, nproc;

int main(int argc, char* argv []) {
    int isbuf, irbuf;
    MPI_Status status;
    
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nproc);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    if (rank == 0) {
        isbuf = 9;
        MPI_Send(&isbuf, 1, MPI_INTEGER, 1, 1, MPI_COMM_WORLD);
    } else if (rank == 1) {
        MPI_Recv(&irbuf, 1, MPI_INTEGER, 0, 1, MPI_COMM_WORLD, &status);
        printf("%d\n", irbuf);
    }
    MPI_Finalize();
}
\end{minted}
\subsubsection{\texttt{MPI\_SENDRECV}}
\begin{minted}{c}
int MPI_Sendrecv(void *sendbuf,         // in
                int sendcount,          // in
                MPI_Datatype sendtype,  // in
                int dest,               // in
                int sendtag,            // in
                void *recvbuf,          // out
                int recvcount,          // out
                MPI_Datatype recvtype,  // out
                int source,             // out
                int recvtag,            // out
                MPI_Comm comm,          // out
                MPI_Status *status );    // out
\end{minted}
\par
Sends and receives a message in a blocking manner.
\subsection{Non-blocking operations}
\par
The procedure may return before the operation completes, and before the user is allowed to reuse resources specified in the call. In other words, when the procedure is called, the execution continues, and it is up to the developer to check for the reception of the message.
\subsubsection{\texttt{MPI\_ISEND}}
\begin{minted}{c}
int MPI_Isend(void* buf,            // in
            int count,              // in
            MPI_Datatype datatype,  // in
            int dest,               // in
            int tag,                // in
            MPI_Comm comm,          // in
            MPI_Request* request ); // out
\end{minted}
\par
Performs a non-blocking send operation. \texttt{request} is an identifier for later enquiry with \texttt{MPI\_WAIT} or \texttt{MPI\_TEST}. The send buffer \texttt{buf} must not be modified until the request has been completed by \texttt{MPI\_WAIT} or \texttt{MPI\_TEST}. The message can be received by either \texttt{MPI\_RECV} or \texttt{MPI\_IRECV}.
\subsubsection{\texttt{MPI\_IRECV}}
\begin{minted}{c}
int MPI_Irecv(void* buf,            // out
            int count,              // in
            MPI_Datatype datatype,  // in
            int source,             // in
            int tag,                // in
            MPI_Comm comm,          // in
            MPI_Request* request ); // out
\end{minted}
\par
Performs a non-blocking receive operation. The receive buffer \texttt{buf} must not be accessed until the receive is completed by \texttt{MPI\_WAIT} or \texttt{MPI\_TEST}. The message received must be less than or equal to the length of the receive buffer \texttt{buf}. The function can receive a message sent by either \texttt{MPI\_SEND} or \texttt{MPI\_ISEND}.
\subsubsection{\texttt{MPI\_WAIT}}
\begin{minted}{c}
int MPI_Wait(MPI_Request* request,  // inout 
            MPI_Status* status );   // out
\end{minted}
\par
Waits for a non-blocking operation to complete, with identifier stored in \texttt{request}. Information on the completed operation is found in \texttt{status}. If wildcards (\texttt{MPI\_ANY\_SOURCE}, \texttt{MPI\_ANY\_TAG}) were used by the recieve for either the source or tag, the actual source and tag can be retrieved from \texttt{status->MPI\_SOURCE} and \texttt{status->MPI\_TAG}.
\subsubsection{\texttt{MPI\_TEST}}
\begin{minted}{c}
int MPI_Test(MPI_Request* request,  // inout
            int* flag,              // out
            MPI_Status* status );    // out
\end{minted}
\par
Test for the completion of a non-blocking send or receive. \texttt{flag} equals \texttt{MPI\_SUCCESS} if MPI routine completed successfully.
\subsubsection{\texttt{MPI\_GET\_COUNT}}
\begin{minted}{c}
int MPI_Get_count(MPI_Status status,    // in
                MPI_Datatype datatype,  // in
                int* count );           // out
\end{minted}
\par
Returns the number of elements in a message (indicated by \texttt{status}). The \texttt{datatype} argument and the argument provided by the call that set the \texttt{status} variable should match.
\subsubsection{\texttt{MPI\_PROBE}}
\begin{minted}{c}
int MPI_Probe(int source,           // in
            int tag,                // in
            MPI_Comm comm,          // in
            MPI_Status *status );   // out
\end{minted}
\par
Blocking call that returns only after a matching message is found. Wildcards can be used to wait for messages coming from any source (\texttt{MPI\_ANY\_SOURCE} or with any tag (\texttt{MPI\_ANY\_TAG}). There also is a non-blocking \texttt{MPI\_Iprobe}.
\subsubsection{Example}
\begin{minted}[breaklines]{c}
#include "mpi.h"

int main(int argc, char* argv[]) {
    int rank, nproc;
    int isbuf, irbuf, count;
    MPI_Request request;
    MPI_Status status;
    
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nproc);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    if (rank == 0) {
        isbuf = 9;
        MPI_Isend(&isbuf, 1, MPI_INTEGER, 1, 1, MPI_COMM_WORLD, &request);
    } else if (rank == 1) {
        MPI_Irecv(&irbuf, 1, MPI_INTEGER, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &request);
        // Other work to do
        MPI_Get_count(status, MPI_INTEGER, &count);
        printf("irbuf = %d, source = %d, tag = %d, count = %d\n", irbuf, status.MPI_SOURCE, status.MPI_TAG, count);
    }
    MPI_Finalize();
}
\end{minted}
\section{MPI data types}
\par
\texttt{MPI\_Datatype} can be one of the following: \texttt{MPI\_CHAR}, \texttt{MPI\_SHORT}, \texttt{MPI\_INT}, \texttt{MPI\_LONG}, \texttt{MPI\_UNSIGNED\_CHAR}, \texttt{MPI\_UNSIGNED\_SHORT}, \texttt{MPI\_UNSIGNED}, \texttt{MPI\_UNSIGNED\_LONG}, \texttt{MPI\_FLOAT}, \texttt{MPI\_DOUBLE}, \texttt{MPI\_LONG\_DOUBLE}, \texttt{MPI\_BYTE}, \texttt{MPI\_PACKED}.
\section{Collective communication}
\par
If all processes in a process group need to invoke the procedure.
\subsection{\texttt{MPI\_BCAST}}
\begin{minted}{c}
int MPI_Bcast(void* buffer,         // inout
            int count,              // in
            MPI_Datatype datatype,  // in
            int root,               // in
            MPI_Comm comm );        // in
\end{minted}
\par
It broadcasts a message from \texttt{root} to all processes in communicator \texttt{comm}. The type signature of \texttt{count} and \texttt{datatype} on any process must be equal to the type signature of \texttt{count} and \texttt{datatype} at the \texttt{root}.
\subsection{\texttt{MPI\_REDUCE}}
\begin{minted}{c}
int MPI_Reduce(void* sendbuf,       // in
            void recvbuf,           // out
            int count,              // in
            MPI_Datatype datatype,  // in
            MPI_Op op,              // in
            int root,               // in
            MPI_Comm comm);         // in
\end{minted}
\par
Applies a reduction operation to the vector \texttt{sendbuf} over the set of processes specified by communicator \texttt{comm} and places the result in \texttt{recvbuf} on \texttt{root}. Both the input and output buffers have the same number of elements with the same type.
\par
Users may define their own operations or use the predefined operations provided by MPI: \texttt{MPI\_\{SUM, PROD, MAX, MIN, MAXLOC, MINLOC, LAND, LOR, LXOR, BAND, BOR, BXOR\}}.
\subsection{Broadcast and reduce: Example}
\begin{minted}[breaklines]{c}
void main(int argc, char* argv[]) {
    int i, rank, nproc, num_steps;
    double x, pi, step, sum = 0.0;
    
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &nproc);
    
    if (rank == 0) {
        scanf("%d", &num_steps);
    }
    MPI_Bcast(&num_steps, 1, MPI_INT, 0, MPI_COMM_WORLD);
    step = 1.0 / (double) num_steps;
    my_steps = num_steps / nproc;
    
    for (i = rank * my_steps; i < (rank + 1) * my_steps; i++) {
        x = (i + 0.5) * step;
        sum += 4.0 / (1.0 + x * x);
    }
    sum *= step;
    MPI_Reduce(&sum, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    MPI_Finalize();
}
\end{minted}
\par
In the code above, only the thread with rank 0 will allow for user input. This user input is the number of steps (iterations). Then, all threads will run the \texttt{MPI\_Bcast} function, with thread ranked 0 as the root. This means that thread of rank 0 will send all the threads its value of the variable \texttt{num\_steps} and those will store it in their \texttt{num\_steps} variable.
\par
The \texttt{for} loop will let each thread to run a certain amount of iterations. Each thread will have its share of \texttt{sum}. With function \texttt{MPI\_Reduce}, all threads will send their \texttt{sum} values to the root (again, thread with rank 0) and, using operation \texttt{MPI\_SUM}, which corresponds to a sum, it will aggregate into one variable.
\subsection{\texttt{MPI\_SCATTER}}
\begin{minted}{c}
int MPI_Scatter(void* sendbuf,          // in
                int sendcount,          // in
                MPI_Datatype sendtype,  // in
                void* recvbuf,          // out
                int recvcount,          // in
                MPI_Datatype recvtype,  // in
                int root,               // in
                MPI_Comm comm );        // in
\end{minted}
\par
It distributes individual messages from \texttt{root} to each process in the communicator. It represents the inverse operation to \texttt{MPI\_GATHER}.
\section{\texttt{MPI\_GATHER}}
\begin{minted}{c}
int MPI_Gather(void* sendbuf,       // in
            int sendcount,          // in
            MPI_Datatype sendtype,  // in
            void* recvbuf,          // out
            int recvcount,          // in
            MPI_Datatype recvtype,  // in
            int root,               // in
            MPI_Comm comm );        // in
\end{minted}
\par
Collects individual messages from each process in the communicator to the \texttt{root} process and stores them in rank order.
\subsection{Scatter and gather: Example}
\begin{minted}[breaklines]{c}
int gsize, localbuf[100];
int root = 0, rank, nproc, *rootbuf;
// ...
MPI_Comm_size(MPI_COMM_WORLD, &nproc);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);

if (rank == root)
    rootbuf = (int *) malloc(nproc * 100 * sizeof(int));

// matrix is initialized in root    
MPI_Scatter(rootbuf, 100, MPI_INT, localbuf, 100, MPI_INT, root, comm);
// do work with data
MPI_Gather(localbuf, 100, MPI_INT, rootbuf, 100, MPI_INT, root, comm);
// results back in root
\end{minted}
\par
In the example above, the root thread will have (supposedly) initialized a matrix, with 100 elements for each thread. These elements, stored in the \texttt{rootbuf}, are distributed, hundred by hundred, into the other threads' \texttt{localbuf} variable. After some work on the data, the content of variable \texttt{localbuf} of all threads is stored back into the root thread's \texttt{rootbuf} variable.
\subsection{\texttt{MPI\_ALLGATHER}}
\begin{minted}{c}
int MPI_Allgather(void* sendbuf,        // in
                int sendcount,          // in
                MPI_Datatype sendtype,  // in
                void* recvbuf,          // out
                int recvcount,          // in
                MPI_Datatype recvtype,  // in
                MPI_Comm comm );        // in
\end{minted}
\par
Does the same as \texttt{MPI\_Gather}, but all threads receive the resulting message.
\subsection{Other variations}
\subsubsection{\texttt{MPI\_SCATTERV}}
\par
Works the same way as \texttt{MPI\_SCATTER}, but permits the user to define how many elements will go to each thread, through an integer array with size \texttt{nproc}.
\subsubsection{\texttt{MPI\_GATHERV}}
\par
The exact opposite of \texttt{MPI\_SCATTERV}, all results are scattered into the root, but it allows to set how many of the elements will be received from each thread.
\subsubsection{\texttt{MPI\_ALLGATHERV}}
\par
Permits to define how many elements will be gathered from each thread, as does \texttt{MPI\_GATHERV}, but all threads will receive the resulting message.
\subsubsection{\texttt{MPI\_ALLTOALL}}
\begin{minted}{c}
int MPI_Alltoall(void* sendbuf,         // in
                int sendcount,          // in
                MPI_Datatype sendtype,  // in
                void* recvbuf,          // out
                int recvcount,          // in
                MPI_Datatype recvtype,  // in
                MPI_Comm comm );        // in
\end{minted}
\par
Sends a distinct message from each process to every other process. The \texttt{j}th block of data sent from process \texttt{i} is received by process \texttt{j} and placed in the \texttt{i}th block of the buffer \texttt{recvbuf}.
\subsubsection{\texttt{MPI\_ALLTOALLV}}
\par
As does \texttt{MPI\_Alltoall}, sends a message from each process to all processes, but allows to specify the amount of elements to send to each thread.
\vfill
\hrule
~\\
Unai Perez Mendizabal \textcopyright \href{https://github.com/unaipme}{https://github.com/unaipme}
\end{multicols}
\newpage\begin{center}
 \Large{\textbf{CPDS: CUDA cheatsheet}} \\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}
\section{Devices, kernel definition and offloading}
\subsection{Device management}
\par
The application can query and select the GPUs.
\begin{itemize}
    \item \texttt{cudaGetDeviceCount(int *count)}
    \item \texttt{cudaSetDevice(int device)}
    \item \texttt{cudaGetDeviceProperties(cudaDeviceProp *prop, int device)}
    \item \texttt{cudaGetDevice(int *device)}
\end{itemize}
\subsection{Offloading kernel execution}
\par
A kernel is a highly parallel function that runs on the device and is called from host. It is defined using the keyword \texttt{\_\_global\_\_}.
\begin{minted}{c}
__global__ void helloworld(void) {
}
\end{minted}
\par
The code shown below offloads the kernel on GPU device, this is, relegates the execution of the kernel function to the said device.
\begin{minted}{c}
helloworld<<<1,1>>>();
\end{minted}
\par
The kernel launch is asynchronous, the host does not wait for the termination of the execution. However, the host may need to wait before continuing execution. Function \texttt{cudaThreadSynchronize()}\footnote{Deprecated, to be replaced by \texttt{cudaDeviceSynchronize()}} blocks the CPU until all CUDA kernels have completed.
\begin{minted}{c}
__global__ void helloworld(void) {
}

int main(void) {
    helloworld<<<1,1>>>();
    cudaThreadSynchronize();
    printf("Hello, world!\n");
    return 0;
}
\end{minted}
\subsubsection{Offloading to multi-GPU nodes}
\par
Any host thread can access all GPUs in the node (if more than one available) using the \texttt{cudaSetDevice} call.
\begin{minted}{c}
int main(void) {
    int numDevs;
    cudaGetDeviceCount(&numDevs);
    
    for (int d = 0; d < numDevs; d++) {
        cudaSetDevice(d);
        helloworld<<<1,1>>>();
    }
    for (int d = 0; d < numDevs; d++) {
        cudaSetDevice(d);
        cudaThreadSynchronize();
    }
    return 0;
}
\end{minted}
\section{Blocks threads and indexing}
\subsection{Parallel kernel}
\par
Kernel replication to use multiple \textbf{streaming multiprocessors} (SM) using the arguments in the kernel launch:
\begin{minted}{c}
// Replication in different SM
helloworld<<<N, 1>>>();
// Replication in different threads inside one SM
helloworld<<<1, N>>>();
\end{minted}
\par
Each kernel instance can be unequivocally identified, which can be used to make kernel instances cooperate. This can also be done at the threads level.
\begin{minted}{c}
__global__ void helloWorld(char* str) {
    // Kernel instance level
    int idx = blockIdx.x;
    // Thread level
    int idx = threadIdx.x;
    
    str[idx] += idx;
}
\end{minted}
\subsection{Indexing vectors with blocks and threads}
\par
Consider indexing an array with one element per thread and $M$ threads per block (e.g. 8). Then a unique index for each thread is given by \texttt{int index = threadIdx.x + blockIdx.x * M}.
\subsubsection{Example: Vector addition}
\par
The sequential code for a vector addition would look like this:
\begin{minted}{c}
// Compute vector sum C = A + B
void vecAdd(float* a, float* b, float* c, int N) {
    for (int i = 0; i < N; i++)
        c[i] = a[i] + b[i];
}

int main() {
    float a[N], b[N], c[N];
    // initialize a and b
    vecAdd(a, b, c, N);
    // display the results
    return 0;
}
\end{minted}
\par
The code for the vector addition with kernel definition and invocation would look like this:
\begin{minted}[breaklines]{c}
// Each kernel invocation performs one pair-wise addition
__global__ void vecAddKernel(float* a_d, float* b_d, float* c_d, int N) {
    int i = threadIdx.x + blockDim.x * blockIdx.x;
    c_d[i] = a_d[i] + b_d[i];
}

__host__ int vectAdd(float* a, float* b, float* c, int N) {
    vecAddKernel<<<ceil(N / 256), 256>>>(a, b, c, N);
}
\end{minted}
\par
In the example above, the vectors are split in blocks of 256 threads and equally distributed among as many blocks as needed. This means that, if the vectors are of size 512, two SM will run blocks of 256 threads.
\par
The \texttt{\_\_global\_\_} function will be run on each of the threads, which will take care of just making the corresponding addition. The \texttt{\_\_host\_\_} function will be called on the host.
\subsection{Function declarations in CUDA}
\begin{itemize}
    \item \texttt{\_\_device\_\_} functions are executed on the device and only callable from the device.
    \item \texttt{\_\_global\_\_} functions (also called kernel functions) are executed on the device and are only callable from the host. They can only return void.
    \item \texttt{\_\_host\_\_} functions are executed and callable only on the host.
\end{itemize}
\subsubsection{Example: vector addition (revisited)}
\par
What if \texttt{blockDim.x} does not divide by $N$?
\begin{minted}[breaklines]{c}
__global__ void vecAddKernel(float* a_d, float* b_d, float* c_d, int N) {
    int i = threadIdx.x + blockDim.x * blockIdx.x;
    if (i < n)
        c_d[i] = a_d[i] + b_d[i];
}
\end{minted}
\subsection{Identifying threads inside a grid}
\par
In general, the GPU offers a grid of threads, divided into thread blocks, and each block divided into threads. The grid of thread blocks can actually be partitioned into 1, 2, or 3 dimensions.
\par
Each thread uses indices (1D, 2D or 3D) to decide what to do and what data to work on (data decomposition). These indices are \texttt{blockIdx} and \texttt{threadIdx}, defined inside a grid with \texttt{gridDim} blocks, each with \texttt{blockDim} threads.
\par
This is, the CUDA grid will have dimensions \texttt{gridDim.x} times \texttt{gridDim.y}. Inside the grid, there will be thread blocks with \texttt{blockDim.x} times \texttt{blockDim.y} threads.
\subsection{\texttt{dim3} datatype}
\par
Data type to define variables to hold block and grid dimensionalities.
\begin{minted}[breaklines]{c}
__host__ int vectAdd(float* a, float* b, float* c, int N) {
    // Run ceil(N/256) blocks of 256 threads each
    dim3 DimGrid(ceil(N/256), 1, 1);
    dim3 DimBlock(256, 1, 1);
    vecAddKernel<<<DimGrid, DimBlock>>>(a, b, c, N);
}
\end{minted}
\subsection{Thread scheduling and execution}
\par
Threads in a block are divided in 32-thread warps (scheduling units in SM). For example, if 3 blocks assigned to an SM, each with 256 threads, how many warps are there in an SM?
\begin{itemize}
    \item Each block is divided into 256 (threads) / 32 (warp size) = 8 warps per block
    \item There are 8 (warps per block) $\times$ 3 (blocks) = 24 warps in total.
\end{itemize}
\par
At any point in time, only one of the 24 warps will be selected for instruction fetch and execution (multithreading).
\section{Accessing (global) memory}
\subsection{Partial view of the device memory}
\par
Device code can read and write thread registers and grid global memory.
\par
Host code can allocate data in grid global memory and transfer data between host and grid global memories.
\subsection{Basic device memory management}
Global memory contents are visible to all threads. Variables in grid global memory can be declared using \texttt{\_\_device\_\_} attribute. It is also dynamically allocatable using \texttt{cudaMalloc} and \texttt{cudaFree}.
\begin{minted}{c}
#define N 1000

// declared outside function and kernel bodies.
__device__ int A[N];

__global__ kernel() {
    int id = blockId.x * blockDim.x + threadId.x;
    A[tid]++;
}
\end{minted}
\begin{itemize}
    \item \texttt{cudaMalloc} allocates an object in the device global memory. It takes two parameters, pointer to the allocated object and its size in bytes.
    \item \texttt{cudaFree} frees an object from device global memory. It takes the pointer to the object as only parameter.
\end{itemize}
\subsubsection{Example: vector addition (host code)}
\begin{minted}[breaklines]{c}
int main() {
    // Arrays in host memory
    float a[N], b [N], c[N];
    // Pointers to vectors dynamically allocated in global memory
    float *dev_a, *dev_b, *dev_c;
    
    cudaMalloc(&dev_a, N * sizeof(float));
    cudaMalloc(&dev_b, N * sizeof(float));
    cudaMalloc(&dev_c, N * sizeof(float));
    
    // ...
    
    // kernel invocation
    dim3 DimGrid(ceil(N/256, 1, 1);
    dim3 DimBlock(256, 1, 1);
    vecAddKernel<<<DimGrid,DimBlock>>>(dev_a, dev_b, dev_c, N);
    
    // ...
    
    // Free the memory allocated on device
    cudaFree(dev_a);
    cudaFree(dev_b);
    cudaFree(dev_c);
    
    return 0;
}
\end{minted}
\subsection{Basic device memory management (cont.)}
\begin{itemize}
    \item \texttt{cudaMemcpy)} is a synchronous function for data transfer. It blocks the CPU until it is complete and the copy does not begin until all preceding CUDA calls have completed. It requires 4 parameters: pointer to destinations, pointer to source, number of bytes to be copied and type of transfer.
    \item \texttt{cudaMemcpyAsync} is the asynchronous version of the function, allowing the overlap of data transfer and computation. It also uses an additional \texttt{stream} parameter that, if not 0, can be used to synchronize.
\end{itemize}
\subsubsection{Vector addition (host code)}
\begin{minted}[breaklines]{c}
int main() {
    // Arrays in host memory
    float a[N], b [N], c[N];
    // Pointers to vectors dynamically allocated in global memory
    float *dev_a, *dev_b, *dev_c;
    
    cudaMalloc(&dev_a, N * sizeof(float));
    cudaMalloc(&dev_b, N * sizeof(float));
    cudaMalloc(&dev_c, N * sizeof(float));
    
    // Initialize a and b in host memory (as sequential)
    
    // Copy a and b from host to device memory
    cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice);
    
    dim3 DimGrid(ceil(N/256), 1, 1);
    dim3 DimBlock(256, 1, 1);
    vecAddKernel<<<DimGrid,DimBlock>>>(dev_a, dev_b, dev_c, N);
    
    // Copy back c from device to host
    cudaMemcpy(c, dev_c, N * sizeof(float), cudaMempcyDeviceToHost);
    
    // Display results
    
    // Free the memory allocated on device
    cudaFree(dev_a);
    cudaFree(dev_b);
    cudaFree(dev_c);
    
    return 0;
}
\end{minted}
\subsection{Sharing data between GPUs}
\par
There are three options to acomplish this.
\begin{itemize}
    \item One is to make copies explicitly from the host code.
    \item Another option is the peer-to-peer memory access. This can be done with the direct copy from pointer on one device to pointer on another device, via the function \texttt{cudaMemcpyPeer(void* dst, int dstDevice, void* src, int srcDevice, size\_t count)}.
    \item The last option is the zero-copy shared host array, allowing the direct access to host memory from the device (Not covered).
\end{itemize}
\section{Cooperating threads and shared memory}
\subsection{Threads vs. blocks}
\par
Unlike blocks, threads have mechanisms to synchronize and communicate via memory. The synchronization can be achieved with the instruction \texttt{\_\_syncthreads()}, which forces all warps (i.e. threads in a block) to wait until they reach the same point.
\subsection{A more complete view of the device memory}
Each thread can:
\begin{itemize}
    \item R/W per-thread registers and local memory.
    \item R/W per-block shared memory (100 times faster than global).
    \item R/W per-grid global memory.
    \item Read only per-grid constant and texture memories.
\end{itemize}
\par
Host code can R/W global, constant and texture memories.
\par
Shared memory is shared and allocated within a block, used to share data among threads and is not visible to threads in other blocks. It is declared using \texttt{\_\_shared\_\_}.
\begin{minted}{c}
#define N 1000

__global__ kernel() {
    __shared__ int A[N];
    int tid = threadId.x;
    A[tid]++;
}
\end{minted}
\subsubsection{Example: 1D stencil}
\par
Consider applying a stencil to a 1D array of elements. Each output element is the sum of input elements within a radius. Each thread processes one output element. Each input element is read $2 \times radius + 1$ times.
\par
Read \texttt{blockDim.x + 2 * radius} input elements from global memory to shared memory. Compute \texttt{blockDim.x} output elements. Write \texttt{blockDim.x} output elements to global memory.
\vfill
\hrule
~\\
Unai Perez Mendizabal \textcopyright \href{https://github.com/unaipme}{https://github.com/unaipme}
\end{multicols}
\end{document}
